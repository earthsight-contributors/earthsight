defaults:
  - _self_
  - model: default_model.yaml 
  - dataset: default_dataset.yaml

project:
  name: mtl_experiment

freeze: full

# Optimizer configuration (shared across experiments)
optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-3
  weight_decay: 0.01
  eps: 1e-8

# Scheduler configuration (shared across experiments)
scheduler:
  _target_: torch.optim.lr_scheduler.StepLR
  step_size: 10
  gamma: 0.1

# Trainer parameters
trainer:
  epochs: 40
  patience: 40
  batch_size: 64

# Checkpoint configuration
# checkpoint_dir: checkpoints   # Directory for checkpoints (project_root/checkpoints)

hydra:
  job_logging:
    disable_existing_loggers: false
    root:
      handlers: []
      level: CRITICAL
    handlers: {}
  output_subdir: null
  run:
    dir: .
